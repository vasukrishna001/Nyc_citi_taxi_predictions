{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2730f8c-f42b-486d-86eb-baab58978dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "2025-05-08 18:55:44,232 INFO: Current date and time (UTC): 2025-05-08 23:00:00\n",
      "2025-05-08 18:55:44,233 INFO: Fetching data from 2025-04-10 23:00:00 to 2025-05-08 23:00:00\n",
      "2025-05-08 18:55:44,234 INFO: Fetching raw data...\n",
      "File already exists for 2024-04.\n",
      "Loading Citi Bike data for 2024-04...\n",
      "Total records: 79,116\n",
      "Valid records: 78,948\n",
      "Records dropped: 168 (0.21%)\n",
      "Successfully processed data for 2024-04.\n",
      "Combining all monthly Citi Bike data...\n",
      "Citi Bike data loading and processing complete!\n",
      "File already exists for 2024-05.\n",
      "Loading Citi Bike data for 2024-05...\n",
      "Total records: 97,479\n",
      "Valid records: 97,225\n",
      "Records dropped: 254 (0.26%)\n",
      "Successfully processed data for 2024-05.\n",
      "Combining all monthly Citi Bike data...\n",
      "Citi Bike data loading and processing complete!\n",
      "2025-05-08 18:55:45,329 INFO: Raw data fetched. Number of records: 82326\n",
      "2025-05-08 18:55:45,330 INFO: Transforming raw data into time-series format...\n",
      "2025-05-08 18:55:45,506 INFO: Sample data:\n",
      "          pickup_hour pickup_location_id  rides          event_hour\n",
      "0 2025-04-10 23:00:00            5187.03      0 2025-04-10 23:00:00\n",
      "1 2025-04-11 00:00:00            5187.03      0 2025-04-11 00:00:00\n",
      "2025-05-08 18:55:45,507 INFO: Schema:\n",
      "pickup_hour           datetime64[ns]\n",
      "pickup_location_id            object\n",
      "rides                          int32\n",
      "event_hour            datetime64[ns]\n",
      "dtype: object\n",
      "2025-05-08 18:55:45,507 INFO: Logging into Hopsworks...\n",
      "2025-05-08 18:55:45,510 INFO: Initializing external client\n",
      "2025-05-08 18:55:45,511 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-05-08 18:55:46,924 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1213633\n",
      "2025-05-08 18:55:47,285 INFO: Successfully connected to Hopsworks project.\n",
      "2025-05-08 18:55:47,286 INFO: Connecting to feature store...\n",
      "2025-05-08 18:55:47,343 INFO: Feature store ready.\n",
      "2025-05-08 18:55:47,344 INFO: Getting feature group: citi_bike_hourly_features (v1)\n",
      "2025-05-08 18:55:47,544 INFO: Inserting data into feature group...\n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Features are not compatible with Feature Group schema: \n - event_hour (type: 'timestamp') does not exist in feature group.\nNote that feature (or column) names are case insensitive and spaces are automatically replaced with underscores.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureStoreException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Step 8: Insert time-series data\u001b[39;00m\n\u001b[32m     77\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mInserting data into feature group...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mts_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwait_for_job\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpartition_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mevent_hour\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mâœ… Data insertion complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\citi_bike25\\Lib\\site-packages\\hsfs\\feature_group.py:3012\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   3009\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._offline_backfill_every_hr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3010\u001b[39m     write_options[\u001b[33m\"\u001b[39m\u001b[33moffline_backfill_every_hr\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._offline_backfill_every_hr\n\u001b[32m-> \u001b[39m\u001b[32m3012\u001b[39m job, ge_report = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3013\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3015\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msave_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3022\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine.get_type().startswith(\u001b[33m\"\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   3025\u001b[39m     \u001b[38;5;66;03m# Also, only compute statistics if stream is False.\u001b[39;00m\n\u001b[32m   3026\u001b[39m     \u001b[38;5;66;03m# if True, the backfill job has not been triggered and the data has not been inserted (it's in Kafka)\u001b[39;00m\n\u001b[32m   3027\u001b[39m     \u001b[38;5;28mself\u001b[39m.compute_statistics()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\citi_bike25\\Lib\\site-packages\\hsfs\\core\\feature_group_engine.py:188\u001b[39m, in \u001b[36mFeatureGroupEngine.insert\u001b[39m\u001b[34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[39m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28mself\u001b[39m.save_feature_group_metadata(\n\u001b[32m    184\u001b[39m         feature_group, dataframe_features, write_options\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# else, just verify that feature group schema matches user-provided dataframe\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_verify_schema_compatibility\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe_features\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# ge validation on python and non stream feature groups on spark\u001b[39;00m\n\u001b[32m    193\u001b[39m ge_report = feature_group._great_expectation_engine.validate(\n\u001b[32m    194\u001b[39m     feature_group=feature_group,\n\u001b[32m    195\u001b[39m     dataframe=feature_dataframe,\n\u001b[32m   (...)\u001b[39m\u001b[32m    198\u001b[39m     ge_type=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    199\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\citi_bike25\\Lib\\site-packages\\hsfs\\core\\feature_group_base_engine.py:186\u001b[39m, in \u001b[36mFeatureGroupBaseEngine._verify_schema_compatibility\u001b[39m\u001b[34m(self, feature_group_features, dataframe_features)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# raise exception if any errors were found.\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(err) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFeatures are not compatible with Feature Group schema: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    188\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m err])\n\u001b[32m    189\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote that feature (or column) names are case insensitive and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    190\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mspaces are automatically replaced with underscores.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    191\u001b[39m     )\n",
      "\u001b[31mFeatureStoreException\u001b[39m: Features are not compatible with Feature Group schema: \n - event_hour (type: 'timestamp') does not exist in feature group.\nNote that feature (or column) names are case insensitive and spaces are automatically replaced with underscores."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import src.config as config\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import hopsworks\n",
    "import pandas as pd\n",
    "\n",
    "import src.config as config\n",
    "from src.data_utils import fetch_batch_citibike_data, transform_raw_data_into_ts_data\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Log format\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),  # Output logs to stdout\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Step 1: Get the current date and time (timezone-aware)\n",
    "current_date = pd.to_datetime(datetime.now(timezone.utc)).ceil(\"h\")\n",
    "logger.info(f\"Current date and time (UTC): {current_date}\")\n",
    "\n",
    "# Step 2: Define the data fetching range\n",
    "fetch_data_to = current_date\n",
    "fetch_data_from = current_date - timedelta(days=28)\n",
    "logger.info(f\"Fetching data from {fetch_data_from} to {fetch_data_to}\")\n",
    "\n",
    "# Step 3: Fetch raw data\n",
    "logger.info(\"Fetching raw data...\")\n",
    "rides = fetch_batch_citibike_data(fetch_data_from, fetch_data_to)\n",
    "logger.info(f\"Raw data fetched. Number of records: {len(rides)}\")\n",
    "\n",
    "# Step 4: Transform raw data into time-series data\n",
    "logger.info(\"Transforming raw data into time-series data...\")\n",
    "ts_data = transform_raw_data_into_ts_data(rides)\n",
    "logger.info(\n",
    "    f\"Transformation complete. Number of records in time-series data: {len(ts_data)}\"\n",
    ")\n",
    "\n",
    "# Step 5: Connect to the Hopsworks project\n",
    "logger.info(\"Connecting to Hopsworks project...\")\n",
    "project = hopsworks.login(\n",
    "    project=config.HOPSWORKS_PROJECT_NAME, api_key_value=config.HOPSWORKS_API_KEY\n",
    ")\n",
    "logger.info(\"Connected to Hopsworks project.\")\n",
    "\n",
    "# Step 6: Connect to the feature store\n",
    "logger.info(\"Connecting to the feature store...\")\n",
    "feature_store = project.get_feature_store()\n",
    "logger.info(\"Connected to the feature store.\")\n",
    "\n",
    "# Step 7: Connect to or create the feature group\n",
    "logger.info(\n",
    "    f\"Connecting to the feature group: {config.FEATURE_GROUP_NAME} (version {config.FEATURE_GROUP_VERSION})...\"\n",
    ")\n",
    "feature_group = feature_store.get_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    ")\n",
    "logger.info(\"Feature group ready.\")\n",
    "\n",
    "# Step 8: Insert data into the feature group\n",
    "logger.info(\"Inserting data into the feature group...\")\n",
    "feature_group.insert(ts_data, write_options={\"wait_for_job\": False})\n",
    "logger.info(\"Data insertion completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35757af-ccaa-4676-b71a-d9232e3ac0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
